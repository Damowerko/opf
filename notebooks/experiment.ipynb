{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from opf.utils import model_from_parameters\n",
    "\n",
    "%aimport opf.utils\n",
    "%aimport opf.modules\n",
    "%aimport opf.powerflow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "params = dict(\n",
    "    case_name=\"case30\",\n",
    "    adj_scaling=\"auto\",\n",
    "    adj_threshold=0.01,\n",
    "    batch_size=256,\n",
    "    max_epochs=1,\n",
    "    patience=1000,\n",
    "    K=8,\n",
    "    F=32,\n",
    "    L=2,\n",
    "    s=10,\n",
    "    t=500,\n",
    "    F_MLP=4,\n",
    "    L_MLP=1,\n",
    "    cost_weight=0.01,\n",
    "    lr=1e-4,\n",
    "    eps=1e-4,\n",
    "    constraint_features=False,\n",
    ")\n",
    "\n",
    "root_dir = \"../\"\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "log_dir = os.path.join(root_dir, \"logs\")\n",
    "\n",
    "logger = WandbLogger(project=\"opf\", save_dir=log_dir, config=params, tags=[\"test\"])\n",
    "barrier, trainer, dm = model_from_parameters(\n",
    "    params,\n",
    "    gpus=1,\n",
    "    logger=logger,\n",
    "    data_dir=data_dir,\n",
    "    patience=params[\"patience\"],\n",
    "    eps=params[\"eps\"],\n",
    "    debug=False\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "trainer.tune(barrier, datamodule=dm)\n",
    "trainer.fit(barrier, dm)\n",
    "trainer.test(datamodule=dm)\n",
    "logger.finalize(\"finished\")\n",
    "wandb.finish()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model                | GNN                 | 124 K \n",
      "1 | powerflow_parameters | PowerflowParameters | 0     \n",
      "-------------------------------------------------------------\n",
      "124 K     Trainable params\n",
      "0         Non-trainable params\n",
      "124 K     Total params\n",
      "0.993     Total estimated model params size (MB)\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:02<00:00, 37.31it/s]Restoring states from the checkpoint file at /home/damow/repos/OPF/notebooks/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/damow/repos/OPF/notebooks/lr_find_temp_model.ckpt\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:02<00:00, 34.25it/s]\n",
      "Learning rate set to 0.0002089296130854041\n",
      "/home/damow/.pyenv/versions/3.9.5/envs/opf/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "/home/damow/.pyenv/versions/3.9.5/envs/opf/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamowerko\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">tough-yogurt-1241</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/damowerko/opf\" target=\"_blank\">https://wandb.ai/damowerko/opf</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/damowerko/opf/runs/2j2r5h82\" target=\"_blank\">https://wandb.ai/damowerko/opf/runs/2j2r5h82</a><br/>\n",
       "                Run data is saved locally in <code>../logs/wandb/run-20210902_110240-2j2r5h82</code><br/><br/>\n",
       "            "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name                 | Type                | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model                | GNN                 | 124 K \n",
      "1 | powerflow_parameters | PowerflowParameters | 0     \n",
      "-------------------------------------------------------------\n",
      "124 K     Trainable params\n",
      "0         Non-trainable params\n",
      "124 K     Total params\n",
      "0.993     Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: 100%|██████████| 392/392 [00:18<00:00, 15.77it/s, loss=233, v_num=5h82, train/loss=230.0, val/loss=230.0]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/damow/.pyenv/versions/3.9.5/envs/opf/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  rank_zero_deprecation(\n",
      "/home/damow/.pyenv/versions/3.9.5/envs/opf/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Testing: 100%|██████████| 798/798 [00:29<00:00, 32.37it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 798/798 [00:29<00:00, 27.25it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 855778<br/>Program ended successfully."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(Label(value=' 1.86MB of 1.86MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "390b012cf0f54efcadfc3476c12b5270"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find user logs for this run at: <code>../logs/wandb/run-20210902_110240-2j2r5h82/logs/debug.log</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find internal logs for this run at: <code>../logs/wandb/run-20210902_110240-2j2r5h82/logs/debug-internal.log</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>238.23734</td></tr><tr><td>train/cost</td><td>0.06403</td></tr><tr><td>train/equality/loss</td><td>45.01468</td></tr><tr><td>train/equality/rate</td><td>1.0</td></tr><tr><td>train/equality/error_mean</td><td>1.1437</td></tr><tr><td>train/equality/error_max</td><td>8.49303</td></tr><tr><td>train/inequality/loss</td><td>193.22202</td></tr><tr><td>train/inequality/rate</td><td>0.32596</td></tr><tr><td>train/inequality/error_mean</td><td>1.37245</td></tr><tr><td>train/inequality/error_max</td><td>9.49049</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>797</td></tr><tr><td>_runtime</td><td>52</td></tr><tr><td>_timestamp</td><td>1630595012</td></tr><tr><td>_step</td><td>805</td></tr><tr><td>val/loss</td><td>230.17435</td></tr><tr><td>val/cost</td><td>0.02748</td></tr><tr><td>val/equality/loss</td><td>39.89295</td></tr><tr><td>val/equality/rate</td><td>1.0</td></tr><tr><td>val/equality/error_mean</td><td>1.11867</td></tr><tr><td>val/equality/error_max</td><td>8.08428</td></tr><tr><td>val/inequality/loss</td><td>190.28113</td></tr><tr><td>val/inequality/rate</td><td>0.32531</td></tr><tr><td>val/inequality/error_mean</td><td>1.37786</td></tr><tr><td>val/inequality/error_max</td><td>9.4905</td></tr><tr><td>test/cost</td><td>5.89246</td></tr><tr><td>test/equality/loss</td><td>0.0</td></tr><tr><td>test/equality/rate</td><td>0.0</td></tr><tr><td>test/equality/error_mean</td><td>0.0</td></tr><tr><td>test/equality/error_max</td><td>0.0</td></tr><tr><td>test/inequality/loss</td><td>14.27426</td></tr><tr><td>test/inequality/rate</td><td>0.09468</td></tr><tr><td>test/inequality/error_mean</td><td>0.47621</td></tr><tr><td>test/inequality/error_max</td><td>1.99082</td></tr><tr><td>acopf/cost</td><td>1.41662</td></tr><tr><td>acopf/equality/loss</td><td>0.0</td></tr><tr><td>acopf/equality/rate</td><td>0.0</td></tr><tr><td>acopf/equality/error_mean</td><td>0.0</td></tr><tr><td>acopf/equality/error_max</td><td>0.0</td></tr><tr><td>acopf/inequality/loss</td><td>0.01607</td></tr><tr><td>acopf/inequality/rate</td><td>0.0</td></tr><tr><td>acopf/inequality/error_mean</td><td>0.0</td></tr><tr><td>acopf/inequality/error_max</td><td>0.0</td></tr></table>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▂▁▁▁▁▁</td></tr><tr><td>train/cost</td><td>▆██▇▅▃▁</td></tr><tr><td>train/equality/loss</td><td>█▂▁▁▁▁▁</td></tr><tr><td>train/equality/rate</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train/equality/error_mean</td><td>█▆▅▃▂▁▁</td></tr><tr><td>train/equality/error_max</td><td>█▃▁▁▄▅█</td></tr><tr><td>train/inequality/loss</td><td>▇██▇▅▃▁</td></tr><tr><td>train/inequality/rate</td><td>█▄▂▂▁▁▁</td></tr><tr><td>train/inequality/error_mean</td><td>█▂▁▁▁▁▂</td></tr><tr><td>train/inequality/error_max</td><td>▁██▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▄▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val/loss</td><td>▁</td></tr><tr><td>val/cost</td><td>▁</td></tr><tr><td>val/equality/loss</td><td>▁</td></tr><tr><td>val/equality/rate</td><td>▁</td></tr><tr><td>val/equality/error_mean</td><td>▁</td></tr><tr><td>val/equality/error_max</td><td>▁</td></tr><tr><td>val/inequality/loss</td><td>▁</td></tr><tr><td>val/inequality/rate</td><td>▁</td></tr><tr><td>val/inequality/error_mean</td><td>▁</td></tr><tr><td>val/inequality/error_max</td><td>▁</td></tr><tr><td>test/cost</td><td>▅▂▂▆▃▄▃▄▄▄█▃▃▄▂▇▁█▂▅▂▄▅▄▄▃▁▅▆▆▆▅▆▆▂▃▁▂▂▅</td></tr><tr><td>test/equality/loss</td><td>▄▂▂▅▁▆▃▂▂▁▁▅▁▅▃▃▂▃▃▂▃▃█▃▃▄▃▁▃▁▂▂▃▂▄▃▁▄▂▂</td></tr><tr><td>test/equality/rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/equality/error_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/equality/error_max</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/inequality/loss</td><td>▆▅▆▂▅▄▄▄▆▇▇▃▄▇▅▆▄█▃▆▄▄▄▇▂▄▄▇█▃▆▄▇█▃▃▃▃▁▃</td></tr><tr><td>test/inequality/rate</td><td>████████████████████████████████████▁▁▁█</td></tr><tr><td>test/inequality/error_mean</td><td>▅▄▄▃▄▂▃▃▆▆▇▄▃▆▄▇▂█▂▇▃▃▅▅▂▃▂▇█▄▆▄▆▇▂▃▃▄▁▄</td></tr><tr><td>test/inequality/error_max</td><td>▆▅▆▂▅▃▄▅▆▅▅▂▅▆▅▆▄▇▃▆▅▄▃▇▃▅▄▇▇▃▅▄▆█▃▃▃▃▁▃</td></tr><tr><td>acopf/cost</td><td>▄▂▁▆▂▃▃▃▄▃█▅▃▄▁▆▁▇▂▇▃▄▆▄▄▃▁▄▆▆▆▅▅▄▂▄▁▃▂▅</td></tr><tr><td>acopf/equality/loss</td><td>▃▂▃▆▃▃▄▄▅▁█▂▆▂▅▆▄▆▃▅▃▂▅▃▆▃▅▅▃▄▃▂▄▁▆▁▄▃▁▄</td></tr><tr><td>acopf/equality/rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>acopf/equality/error_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>acopf/equality/error_max</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>acopf/inequality/loss</td><td>▂▆▃▃▆▁▂▁█▆▄█▂▇▇▇▂▅▃█▃▁█▃▂▂▃▇█▂█▃▃▂▃▆▂▄▁▃</td></tr><tr><td>acopf/inequality/rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>acopf/inequality/error_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>acopf/inequality/error_max</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">tough-yogurt-1241</strong>: <a href=\"https://wandb.ai/damowerko/opf/runs/2j2r5h82\" target=\"_blank\">https://wandb.ai/damowerko/opf/runs/2j2r5h82</a><br/>\n",
       "                "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('opf': pyenv)"
  },
  "interpreter": {
   "hash": "e5fbda20988e957388c9a3a00dcb9ed41bc70017dbd51a8f0ab69eb07c4807c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}